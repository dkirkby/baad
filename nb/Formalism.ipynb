{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Addition of Astronomical Data: Formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine individual observations of 1D or 2D binned data that satisfy the following conditions:\n",
    " - Each observation views the same static source.\n",
    " - Each observation is convolved with a known (possibly varying) point-spread function.\n",
    " - Observed data is accumulated into bins (pixels) with known edges on a regular (not necessarily uniform) grid.\n",
    " - Statistical uncertainties in bin values are well described by a known Gaussian\n",
    "   inverse covariance matrix (not necessarily diagonal).\n",
    " - Missing or bad data is flagged with zero inverse variance.\n",
    "\n",
    "The joint likelihood of all observations is accumulated using the sufficient summary\n",
    "statistics proposed in Kaiser 2004 \"Addition of Images with Varying Seeing\" (unpublished\n",
    "technical note). Observations are then combined in a Bayesian framework with a Gaussian\n",
    "prior having a single hyperparameter. (An equivalent view of this procedure is that the\n",
    "hyperparameter regularizes the extraction of high-frequency information that has been\n",
    "erased by PSF convolution.) Methods are provided to calculate the data evidence\n",
    "as a function of this hyperparameter, and to support model optimization or averaging.\n",
    "\n",
    "An extracted \"coadd\" takes the form of a multivariate Gaussian posterior probability\n",
    "density (specifed with a mean vector and covariance matrix) in parameters that are\n",
    "arbitrary linear combinations of the true flux tabulated on a high-resolution grid.\n",
    "Convenience methods are provided for:\n",
    " - Estimated true flux downsampled to square pixels.\n",
    " - Estimated true flux convolved with a Gaussian PSF of fixed size.\n",
    " - Estimated true flux convolved with an effective PSF that whitens the noise.\n",
    "\n",
    "Extracted coadds are numerically stable and well defined within a Bayesian framework\n",
    "for any sequence of observations.  With a suitable choice of extracted pixel or\n",
    "Gaussian PSF width, the results are also insensitive to the choice of hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation $D_j$, indexed by $j$, consists of $n_j$ pixels.  The formalism for 1D and 2D data is essentially the same once we \"flatten\" the row and column indices for 2D pixels down to a single data-vector index $i$.\n",
    "\n",
    "The observed pixels are specified by:\n",
    " - A vector $\\mathbf{d}_j$ of $n_j$ flux estimates,\n",
    " - An $n_j\\times n_j$ covariance matrix $C_j$ of (possibly correlated) statistical uncertainties in these flux estimates,\n",
    " - An $n_j\\times n_j$ matrix of calibration factors $F_j$ to account for varying exposure time, throughput, crosstalk, etc,\n",
    "\n",
    "The pixel boundaries lie on a known regular grid:\n",
    " - 1D: boundaries are specified by an array of increasing edge values (usually in wavelength).\n",
    " - 2D: boundaries are specified by separate row and column arrays of increasing edge values (usually in length or angular offsets).\n",
    "\n",
    "The PSF is \"known\", meaning that we can tabulate the support $\\mathbf{g}_{ij}$ of each observed pixel $i$ in each observation $j$ as the vector of probabilities that true flux disperses into the pixel from each point on a high-resolution grid.  If we assume that the PSF varies slowly between neighboring pixels, then we can approximate this support as the convolution of each pixel's \"top-hat\" response with the PSF at its center. The high-resolution grid has a (flattened) size $N \\gg n_i$, that extends beyond all observations, allowing for dispersion by the largest expected PSF. In practice, tabulated PSFs are truncated and renormalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $N$ model parameters $\\mathbf{f}$ are the true (i.e., calibrated and undispersed) source fluxes tabulated on the same high-resolution grid as the per-pixel PSFs.  The predicted mean flux for observation $j$ is then:\n",
    "$$\n",
    "\\langle \\mathbf{d}_j\\rangle = F_j G_j\\,\\mathbf{f}\n",
    "$$\n",
    "where $G_j$ is the $n_j\\times N$ matrix whose rows are the support vectors $\\mathbf{g}_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood of observation $j$ is then\n",
    "$$\n",
    "\\log P(D_j \\mid \\mathbf{f}) = -\\frac{n_j}{2} \\log(2\\pi) -\\frac{1}{2}\\log|C_j|\n",
    "-\\frac{1}{2} \\left(\\mathbf{d}_j - F_j G_j\\,\\mathbf{f}\\right)^t C_j^{-1} \\left(\\mathbf{d}_j - F_j G_j\\,\\mathbf{f}\\right) \\; .\n",
    "$$\n",
    "A dataset $D$ consisting of $M$ independent observations indexed by $j$, has the joint likelihood\n",
    "$$\n",
    "\\log P(D \\mid \\mathbf{f}) = -\\frac{\\sum_j n_j}{2} \\log(2\\pi) -\\frac{1}{2} \\sum_{j=1}^M\\left[\n",
    "\\log|C_j| + \n",
    "\\left(\\mathbf{d_j} - F_j G_j\\,\\mathbf{f}\\right)^t C_j^{-1} \\left(\\mathbf{d_j} - F_j G_j\\,\\mathbf{f}\\right)\n",
    "\\right]\\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose a Gaussian prior on the parameters $\\mathbf{f}$,\n",
    "$$\n",
    "\\log P(\\mathbf{f}\\mid \\sigma) = -\\frac{N}{2}\\log(2\\pi) -N \\log\\sigma -\\frac{1}{2} \\sigma^{-2} \\mathbf{f}^t \\mathbf{f} \\; ,\n",
    "$$\n",
    "where $\\sigma$ is a hyperparameter of the prior.  This prior states that, in the absence of any observations, the expected source flux is zero with a variance $\\sigma^2$ per high-resolution grid element.  This is not particularly physically motivated but serves as an effective regularization of an ill-conditioned extraction.  Extracted results should normally not depend on the choice of prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the likelihood and prior to evaluate the joint probability density over the data and parameters:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log P(D, \\mathbf{f}\\mid \\sigma) &= \\log P(D\\mid \\mathbf{f}) + \\log P(\\mathbf{f}\\mid \\sigma) \\\\\n",
    "&=\n",
    "-\\frac{\\sum_j n_j+N}{2}\\log(2\\pi) - N \\log\\sigma\n",
    "-\\frac{1}{2} \\sum_{j=1}^M \\log|C_j|\\\\\n",
    "&\\quad -\\frac{1}{2}\\sigma^{-2} \\mathbf{f}^t \\mathbf{f}\n",
    "-\\frac{1}{2} \\sum_{j=1}^M\n",
    "\\left(\\mathbf{d_j} - F_j G_j\\,\\mathbf{f}\\right)^t C_j^{-1} \\left(\\mathbf{d_j} - F_j G_j\\,\\mathbf{f}\\right) \\\\\n",
    "&=\n",
    "-\\frac{\\sum_j n_j+N}{2}\\log(2\\pi) - N \\log\\sigma\n",
    "-\\frac{1}{2} \\sum_{j=1}^M \\log|C_j|\\\\\n",
    "&\\quad -\\frac{1}{2} \\sum_{j=1}^M \\mathbf{d_j}^t C_j^{-1} \\mathbf{d_j}\n",
    "-\\frac{1}{2} \\mathbf{f}^t A(\\sigma)\\,\\mathbf{f}\n",
    "+\\mathbf{f}^t \\boldsymbol{\\varphi} \\\\\n",
    "&=\n",
    "-\\frac{\\sum_j n_j+N}{2}\\log(2\\pi) - N \\log\\sigma\n",
    "-\\frac{1}{2} \\sum_{j=1}^M \\log|C_j|\\\\\n",
    "&\\quad -\\frac{1}{2} \\sum_{j=1}^M \\mathbf{d_j}^t C_j^{-1} \\mathbf{d_j}\n",
    "-\\frac{1}{2} \\left( \\mathbf{f} - A(\\sigma)^{-1}\\boldsymbol{\\varphi}\\right)^t A(\\sigma)\n",
    "\\left( \\mathbf{f} - A(\\sigma)^{-1}\\boldsymbol{\\varphi}\\right)\n",
    "+\\frac{1}{2}\\boldsymbol{\\varphi}^t A(\\sigma)^{-1}\\,\\boldsymbol{\\varphi}\n",
    "\\end{aligned}\n",
    "$$\n",
    "with the contributions of the individual exposures only appearing in the summary statistics\n",
    "$$\n",
    "A(\\sigma)\\equiv \\sum_{j=1}^M G_j^t F_j^t C_j^{-1} F_j G_j + \\sigma^{-2}\\mathbb{1}_N \\quad , \\quad\n",
    "\\boldsymbol{\\varphi} \\equiv \\sum_{j=1}^M G_j^t F_j^t C_j^{-1} \\mathbf{d_j} \\; .\n",
    "$$\n",
    "These two quantities are [sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic) since they contain all the information about the individual exposures necessary to evaluate the full joint likelihood (and therefore also the posterior). The limited support $\\mathbf{g}_{ij}$ of each pixel leads to a sparse structure for the $N\\times N$ matrix $A(\\sigma)$, with a fraction of non-zero elements $\\simeq N_{\\rm PSF} / N$ where $N_{\\rm PSF}$ is the typical number of non-zero elements in each $\\mathbf{g}_{ij}$.  The (much smaller) vector $\\boldsymbol{\\varphi}$ is generally dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence and Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate over $\\mathbf{f}$ to calculate the log-evidence given the hyperparameter $\\sigma$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log P(D\\mid\\sigma) &= \\log \\int P(D, \\mathbf{f}\\mid\\sigma)\\,d\\,\\mathbf{f} \\\\\n",
    "&= -\\frac{\\sum_j n_j+2N}{2}\\log(2\\pi)\n",
    "-\\frac{1}{2} \\sum_{j=1}^M \\mathbf{d_j}^t C_j^{-1} \\mathbf{d_j}\n",
    "-\\frac{1}{2} \\sum_{j=1}^M \\log|C_j|\n",
    "-N \\log\\sigma\n",
    "-\\frac{1}{2}\\log|A(\\sigma)|\n",
    "+\\frac{1}{2}\\boldsymbol{\\varphi}^t A(\\sigma)^{-1}\\,\\boldsymbol{\\varphi} \\\\\n",
    "&= \\text{constant} -N \\log\\sigma -\\frac{1}{2}\\log|A(\\sigma)|\n",
    "+\\frac{1}{2}\\boldsymbol{\\varphi}^t A(\\sigma)^{-1}\\,\\boldsymbol{\\varphi}\n",
    "\\; ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where only the last three terms depend on $\\sigma$. The normalized log-posterior is then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log P(\\mathbf{f}\\mid D, \\sigma) &= \\log P(D\\mid \\mathbf{f}) + \\log P(\\mathbf{f}\\mid \\sigma) - \\log P(D\\mid\\sigma) \\\\\n",
    "&= -\\frac{N}{2}\\log(2\\pi) + \\frac{1}{2}\\log |A(\\sigma)|\n",
    "-\\frac{1}{2} \\left( \\mathbf{f} - A(\\sigma)^{-1}\\boldsymbol{\\varphi}\\right)^t A(\\sigma)\n",
    "\\left( \\mathbf{f} - A(\\sigma)^{-1}\\boldsymbol{\\varphi}\\right) \\; .\n",
    "\\end{aligned}\n",
    "$$\n",
    "Empirical studies show that the most probable $\\sigma$ scales with the high-resolution grid spacing $\\propto N^{-1}$ but only mildy with the data pixel size and uncertainty, or the number of exposures in the coadd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formal solution to the maximum-likelihood (ML) problem,\n",
    "$$\n",
    "\\mathbf{f}_{\\rm ML} = A(0)^{-1} \\boldsymbol{\\varphi} \\; ,\n",
    "$$\n",
    "is generally ill-conditioned since it uses a deconvolution to attempt to reconstruct high-frequency information that has been erased by the PSFs. On the other hand, the maximum-a-posteori (MAP) problem has a well behaved solution,\n",
    "$$\n",
    "\\mathbf{f}_{\\rm MAP} = A(\\sigma)^{-1} \\boldsymbol{\\varphi} \\; ,\n",
    "$$\n",
    "in which the hyperparameter $\\sigma$ effectively regularizes the ML problem, but biases the high-frequency reconstruction.  Note that a sparse $A(\\sigma)$ does not translate into a sparse inverse $A(\\sigma)^{-1}$, so it is generally more efficient to obtain $\\mathbf{f}$ as the solution to the sparse linear system\n",
    "$$\n",
    "A(\\sigma)\\,\\mathbf{f}_{\\rm MAP} = \\boldsymbol{\\varphi} \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than estimate $\\mathbf{f}$, we can perform a linear change of variables\n",
    "$$\n",
    "\\mathbf{h} = H \\mathbf{f}\n",
    "$$\n",
    "to extract $P < N$ parameters that filter out the poorly constrained high-frequency information, where $H$ has dimensions $P\\times N$. An estimation of $\\mathbf{h}$ should then be insensitive to the choice of $\\sigma$. For example, $H$ could implement downsampling to a predefined pixel grid or convolution with a predefined (truncated) PSF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this change of variables, we first embed $H$ in an $N\\times N$ square invertible matrix $K$ as:\n",
    "$$\n",
    "K_{ij} = \\begin{cases}\n",
    "H_{k_p j} & p = 1,\\ldots, P\\\\\n",
    "\\delta_{ij} & \\rm{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "In other words, $K$ is an identity matrix with row $k_p$ replaced by row $p$ of $H$. The $k_p$ must each be different and chosen so that $K$ is invertible.  In practice, setting\n",
    "$$\n",
    "k_p = \\underset{i}{\\operatorname{argmax}} H_{pi}\n",
    "$$\n",
    "works in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-posterior PDF $P_k$ for $\\mathbf{k} = K \\mathbf{f}$ is then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log P_k(\\mathbf{k}\\mid D, \\sigma) &= \\log |K^{-1}| + \\log P(\\mathbf{f} = K^{-1}\\mathbf{k} \\mid D, \\sigma) \\\\\n",
    "&= -\\frac{N}{2}\\log(2\\pi) + \\frac{1}{2}\\log \\left|K^{-t} A(\\sigma) K^{-1}\\right|\n",
    "-\\frac{1}{2} \\left( \\mathbf{k} - (A(\\sigma) K^{-1})^{-1}\\boldsymbol{\\varphi}\\right)^t K^{-t} A(\\sigma) K^{-1}\n",
    "\\left( \\mathbf{k} - (A(\\sigma) K^{-1})^{-1}\\boldsymbol{\\varphi}\\right) \\; .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the mean value $\\langle\\mathbf{k}\\rangle$ is the solution to\n",
    "$$\n",
    "A(\\sigma) K^{-1}\\,\\mathbf{k} = \\boldsymbol{\\varphi}\n",
    "$$\n",
    "and $\\langle\\mathbf{h}\\rangle$ are its elements at each $k_p$.  We calculate the covariance $C_h$ of the estimated $\\mathbf{h}$ by marginalizing over the $N-P$ nuiscance parameters of $\\mathbf{k}$. Since the posterior is Gaussian in $\\mathbf{k}$, this simply involves restricting the full $\\mathbf{k}$ covariance,\n",
    "$$\n",
    "C_k = K A(\\sigma)^{-1} K^t\n",
    "$$\n",
    "to the rows and columns at each $k_p$.  In practice, we obtain $C_k$ indirectly as the solution to the linear system\n",
    "$$\n",
    "A(\\sigma) K^{-1} C_k = K^t \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the summary statistics, $A(\\sigma)$ and $\\boldsymbol{\\varphi}$, and downsampling scheme, $H$, only enter the calculation of $\\langle\\mathbf{k}\\rangle$ and $C_k$ through the combination $A(\\sigma) K^{-1}$. To evaluate this combination efficiently, we note that the rows and columns of $K$ can be permuted so that it has the block diagonal form\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "H_1 & H_2 \\\\\n",
    "\\mathbb{0}_{(N-P)\\times P} & \\mathbb{1}_{N-P}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "with inverse\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "H_1^{-1} & -H_1^{-1} H_2 \\\\\n",
    "\\mathbb{0}_{(N-P)\\times P} & \\mathbb{1}_{N-P}\n",
    "\\end{bmatrix} \\; ,\n",
    "$$\n",
    "where $H_1$ and $H_2$ partition the columns of the original $H$ and the sparse matrix $H_1$ will generally have a sparse inverse $H_1^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, any extracted results are insensitive to the choice of hyperparameter $\\sigma$.  Any sensitivity can be empirically measured by performing several extractions with different hyperparameters. When there is some residual sensitivity to $\\sigma$, there are two options: use the value preferred by the data (as measured by the evidence $P(D\\mid \\sigma)$, or marginalize over the space of models.\n",
    "\n",
    "The first approach involves find a solution (with standard root-finding numerical methods) to\n",
    "$$\n",
    "\\frac{d}{d\\log\\sigma} \\log P(D\\mid \\sigma) = 0 \\; ,\n",
    "$$\n",
    "where we can simplify\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{d\\log\\sigma} \\log P(D\\mid \\sigma) &=\n",
    "\\sigma \\frac{d}{d\\sigma} \\log P(D\\mid \\sigma) \\\\\n",
    "&= -N - \\frac{\\sigma}{2|A(\\sigma)|} \\frac{d}{d\\sigma}|A(\\sigma)|\n",
    "+ \\frac{\\sigma}{2} \\boldsymbol{\\varphi}^t \\frac{d}{d\\sigma} A^{-1}(\\sigma) \\boldsymbol{\\varphi} \\\\\n",
    "&= -N - \\frac{\\sigma}{2} \\text{tr}\\left( A^{-1}(\\sigma) \\frac{d}{d\\sigma} A(\\sigma)\\right)\n",
    "-\\frac{\\sigma}{2} \\boldsymbol{\\varphi}^t A^{-1}(\\sigma) \\frac{d}{d\\sigma} A(\\sigma)\\,A^{-1}(\\sigma)\\,\\boldsymbol{\\varphi} \\\\\n",
    "&= -N + \\sigma^{-2} \\left[ \\text{tr}\\, A^{-1}(\\sigma)\n",
    "+ \\boldsymbol{\\varphi}^t A^{-2}(\\sigma)\\,\\boldsymbol{\\varphi}\\right] \\; ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "using\n",
    "$$\n",
    "\\frac{d}{d\\sigma} A(\\sigma) = -2 \\sigma^{-3} \\mathbb{1}_N \\; .\n",
    "$$\n",
    "\n",
    "The second approach involves..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
